# import requests
# from bs4 import BeautifulSoup
# import os
# import json
# import faiss
# import torch
# import numpy as np
# from PIL import Image, ImageFile
# from io import BytesIO
# from transformers import CLIPProcessor, CLIPModel

# # PIL ë””ì½”ë”© ì¶©ëŒ ë°©ì§€
# ImageFile.LOAD_TRUNCATED_IMAGES = True

# # ğŸ”¹ CLIP ëª¨ë¸ ì´ˆê¸°í™” (CPU ì‚¬ìš© ê°•ì œ)
# device = "cpu"
# clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
# clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
# clip_model.to(device)
# clip_model.eval()

# # ğŸ”¹ ì¸ë„¤ì¼ URL â†’ ì›ë³¸ ì´ë¯¸ì§€ URL ë³€í™˜
# def convert_to_original_image_url(thumbnail_url: str) -> str:
#     if "/thumb/" in thumbnail_url:
#         base_url, thumb_path = thumbnail_url.split("/thumb/")
#         parts = thumb_path.split("/")
#         original_path = "/".join(parts[:-1])
#         return f"{base_url}/{original_path}"
#     return thumbnail_url

# # ğŸ”¹ ì´ë¯¸ì§€ URLì—ì„œ PIL ì´ë¯¸ì§€ ë¡œë”©
# def load_image_from_url(url: str) -> Image.Image:
#     headers = {
#         "User-Agent": "EyediaBot/1.0 (Eyedia script; contact: kangchaewon@example.com)"
#     }
#     response = requests.get(url, headers=headers)
#     response.raise_for_status()
#     img = Image.open(BytesIO(response.content)).convert("RGB")
#     img.load()
#     return img

# # ğŸ”¹ ì´ë¯¸ì§€ â†’ CLIP ì„ë² ë”© (ì„¸ê·¸í´íŠ¸ ë°©ì§€ ë²„ì „)
# def embed_image_from_url(url: str):
#     print(f"[ğŸ”] ì´ë¯¸ì§€ ë¡œë”© ì‹œë„: {url}")
#     image = load_image_from_url(url).resize((224, 224))
#     print("[âœ…] ì´ë¯¸ì§€ ë¡œë”© ì„±ê³µ")

#     inputs = clip_processor(images=image, return_tensors="pt")
#     pixel_values = inputs["pixel_values"]

#     print(f"[â„¹ï¸] pixel_values dtype: {pixel_values.dtype}, shape: {pixel_values.shape}")
#     if pixel_values.dtype != torch.float32:
#         pixel_values = pixel_values.to(dtype=torch.float32)

#     pixel_values = pixel_values.to(device)

#     with torch.no_grad():
#         image_features = clip_model.get_image_features(pixel_values=pixel_values)

#     image_features = image_features / image_features.norm(dim=-1, keepdim=True)
#     return image_features.cpu().numpy().astype("float32")

# # ğŸ”¹ ìœ„í‚¤ë°±ê³¼ì—ì„œ ì œëª©, ì´ë¯¸ì§€(ì›ë³¸), ì„¤ëª… ì¶”ì¶œ
# def fetch_wikipedia_info(title: str, section_id: str = "ì„¤ëª…", lang: str = "ko"):
#     url = f"https://{lang}.wikipedia.org/wiki/{title}"
#     headers = {
#         "User-Agent": "EyediaBot/1.0 (Eyedia script; contact: kangchaewon@example.com)"
#     }
#     response = requests.get(url, headers=headers)
#     if response.status_code != 200:
#         print(f"[âŒ] '{title}' ìš”ì²­ ì‹¤íŒ¨")
#         return None

#     soup = BeautifulSoup(response.text, "html.parser")
#     clean_title = title.replace("_", " ")

#     # infobox ë‚´ ì´ë¯¸ì§€ URL
#     image_url = None
#     infobox = soup.find("table", class_="infobox")
#     if infobox:
#         img = infobox.find("img")
#         if img:
#             raw_url = "https:" + img["src"]
#             image_url = convert_to_original_image_url(raw_url)

#     # ì„¤ëª… ì„¸íŠ¸ì…˜ íŒŒì‹±
#     paragraphs = []
#     for h2 in soup.find_all("h2"):
#         span = h2.find("span", {"id": section_id})
#         if span:
#             for sibling in h2.find_next_siblings():
#                 if sibling.name == "h2":
#                     break
#                 if sibling.name == "p":
#                     paragraphs.append(sibling.get_text(strip=True))
#             break

#     return {
#         "title": clean_title,
#         "image_url": image_url,
#         "description": "\n\n".join(paragraphs) if paragraphs else "[ì„¤ëª… ì—†ìŒ]"
#     }

# # ğŸ”¹ FAISS ì¸ë±ìŠ¤ + ë©”íƒ€ì •ë³´ ì €ì¥
# def save_image_to_faiss(info, index_path, meta_path):
#     if not info["image_url"]:
#         print(f"[âŒ] ì´ë¯¸ì§€ ì—†ìŒ: {info['title']}")
#         return
#     try:
#         emb = embed_image_from_url(info["image_url"])[0]
#     except Exception as e:
#         print(f"[âŒ] ì´ë¯¸ì§€ ì„ë² ë”© ì‹¤íŒ¨: {info['title']} â†’ {e}")
#         return

#     # FAISS ì¸ë±ìŠ¤ ìƒì„±
#     dim = emb.shape[0]
#     if os.path.exists(index_path):
#         index = faiss.read_index(index_path)
#     else:
#         index = faiss.IndexFlatIP(dim)
#     index.add(np.array([emb]))
#     faiss.write_index(index, index_path)

#     # ë©”íƒ€ì •ë³´ JSON ì €ì¥
#     meta = []
#     if os.path.exists(meta_path):
#         with open(meta_path, "r", encoding="utf-8") as f:
#             meta = json.load(f)

#     meta.append({
#         "title": info["title"],
#         "image_url": info["image_url"],
#         "description": info["description"]
#     })

#     with open(meta_path, "w", encoding="utf-8") as f:
#         json.dump(meta, f, indent=2, ensure_ascii=False)

#     print(f"âœ… '{info['title']}' ì´ë¯¸ì§€ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")

# # âœ… ì‹¤í–‰ ì˜ˆì‹œ
# titles = [
#     "ì†Œí¬ë¼í…ŒìŠ¤ì˜_ì£½ìŒ",
#     "ë¯¼ì¤‘ì„_ì´ëëŠ”_ììœ ì˜_ì—¬ì‹¬",
#     "ì§„ì£¼_ê·€ê±°ë¦¬ë¥¼_í•œ_ì†Œë…€",
#     "ê·¸ë‘ë“œ_ìíŠ¸ì„¬ì˜_ì¼ìš”ì¼_ì˜¤í›„",
#     "ë¼ìŠ¤_ë©”ë‹ˆë‚˜ìŠ¤"
# ]

# for title in titles:
#     print(f"ğŸ“¦ '{title.replace('_', ' ')}' ì„ë² ë”© ì‹œë„ ì¤‘...")
#     info = fetch_wikipedia_info(title)
#     if info:
#         save_image_to_faiss(
#             info,
#             index_path="./data/faiss/wiki_text.index",
#             meta_path="./data/faiss/wiki_text_meta.json"
#         )
import requests
from bs4 import BeautifulSoup
import os
import json
import faiss
import torch
import numpy as np
from transformers import CLIPProcessor, CLIPModel

# ğŸ”¹ CLIP ëª¨ë¸ ì´ˆê¸°í™” (í…ìŠ¤íŠ¸ ì „ìš©)
device = "cpu"
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model.to(device)
clip_model.eval()

# ğŸ”¹ ì¸ë„¤ì¼ URL â†’ ì›ë³¸ ì´ë¯¸ì§€ URL ë³€í™˜
def convert_to_original_image_url(thumbnail_url: str) -> str:
    if "/thumb/" in thumbnail_url:
        base_url, thumb_path = thumbnail_url.split("/thumb/")
        parts = thumb_path.split("/")
        original_path = "/".join(parts[:-1])
        return f"{base_url}/{original_path}"
    return thumbnail_url

# ğŸ”¹ ìœ„í‚¤ë°±ê³¼ì—ì„œ ì œëª©, ì´ë¯¸ì§€(ì›ë³¸), ì„¤ëª… ì¶”ì¶œ
def fetch_wikipedia_info(title: str, section_id: str = "ì„¤ëª…", lang: str = "ko"):
    url = f"https://{lang}.wikipedia.org/wiki/{title}"
    headers = {
        "User-Agent": "EyediaBot/1.0 (Eyedia script; contact: kangchaewon@example.com)"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"[âŒ] '{title}' ìš”ì²­ ì‹¤íŒ¨ (status {response.status_code})")
        return None

    soup = BeautifulSoup(response.text, "html.parser")
    clean_title = title.replace("_", " ")

    # infobox ë‚´ ì´ë¯¸ì§€ URL
    image_url = None
    infobox = soup.find("table", class_="infobox")
    if infobox:
        img = infobox.find("img")
        if img:
            raw_url = "https:" + img["src"]
            image_url = convert_to_original_image_url(raw_url)

    # ì„¤ëª… ì„¹ì…˜ íŒŒì‹± (í–¥ìƒëœ ë°©ì‹)
    paragraphs = fetch_wikipedia_section_by_id(title, section_id, lang)

    # ì„¤ëª…ì´ ì—†ì„ ê²½ìš° ì²« ë²ˆì§¸ <p> 2~3ê°œ ì¶”ì¶œí•˜ì—¬ ëŒ€ì²´
    if not paragraphs:
        all_paragraphs = soup.find_all("p")
        for p in all_paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 50:
                paragraphs.append(text)
            if len(paragraphs) >= 3:
                break

    return {
        "title": clean_title,
        "image_url": image_url,
        "description": "\n\n".join(paragraphs) if paragraphs else "[ì„¤ëª… ì—†ìŒ]"
    }

# ğŸ”¹ íŠ¹ì • ì„¹ì…˜ì˜ ë¬¸ë‹¨ ì¶”ì¶œ
def fetch_wikipedia_section_by_id(title: str, section_id: str = "ì„¤ëª…", lang: str = "ko") -> list:
    url = f"https://{lang}.wikipedia.org/wiki/{title}"
    headers = {
        "User-Agent": "EyediaBot/1.0 (Eyedia script; contact: kangchaewon@example.com)"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    target_h2 = soup.find("span", {"id": section_id})
    if not target_h2:
        return []

    section_paragraphs = []
    for sibling in target_h2.parent.find_next_siblings():
        if sibling.name == "h2":
            break
        if sibling.name == "p":
            section_paragraphs.append(sibling.get_text(strip=True))
    return section_paragraphs

# ğŸ”¹ í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text: str):
    inputs = clip_processor(text=[text], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        emb = clip_model.get_text_features(**inputs)
    emb = emb / emb.norm(dim=-1, keepdim=True)
    return emb.cpu().numpy().astype("float32")

# ğŸ”¹ FAISS ì¸ë±ìŠ¤ + ë©”íƒ€ ì €ì¥
def save_text_to_faiss(info, index_path, meta_path):
    if not info["description"] or info["description"] == "[ì„¤ëª… ì—†ìŒ]":
        print(f"[âŒ] ì„¤ëª… ì—†ìŒ: {info['title']}")
        return
    try:
        emb = embed_text(info["description"])[0]
    except Exception as e:
        print(f"[âŒ] í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {info['title']} â†’ {e}")
        return

    # FAISS ì¸ë±ìŠ¤
    dim = emb.shape[0]
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
    else:
        index = faiss.IndexFlatIP(dim)
    index.add(np.array([emb]))
    faiss.write_index(index, index_path)

    # ë©”íƒ€ ì •ë³´ ì €ì¥
    meta = []
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

    meta.append({
        "title": info["title"],
        "image_url": info["image_url"],
        "description": info["description"]
    })

    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)

    print(f"âœ… '{info['title']}' í…ìŠ¤íŠ¸ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")

# âœ… ì‹¤í–‰ ì˜ˆì‹œ
titles = [
    "ì†Œí¬ë¼í…ŒìŠ¤ì˜_ì£½ìŒ",
    "ë¯¼ì¤‘ì„_ì´ë„ëŠ”_ììœ ì˜_ì—¬ì‹ ",
    "ì§„ì£¼_ê·€ê±¸ì´ë¥¼_í•œ_ì†Œë…€",
    "ê·¸ë‘ë“œ_ìíŠ¸ì„¬ì˜_ì¼ìš”ì¼_ì˜¤í›„",
    "ë¼ìŠ¤_ë©”ë‹ˆë‚˜ìŠ¤"
]

for title in titles:
    print(f"ğŸ“¦ '{title.replace('_', ' ')}' ì„ë² ë”© ì‹œë„ ì¤‘...")
    info = fetch_wikipedia_info(title)
    if info:
        save_text_to_faiss(
            info,
            index_path="./data/faiss/wiki_text.index",
            meta_path="./data/faiss/wiki_text_meta.json"
        )