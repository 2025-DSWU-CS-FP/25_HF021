import requests
from bs4 import BeautifulSoup

def fetch_wikipedia_info_with_section(title: str, section_id: str = "ì„¤ëª…", lang: str = "ko"):
    """
    ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œì—ì„œ ì œëª©, ëŒ€í‘œ ì´ë¯¸ì§€ URL, ì£¼ì–´ì§„ ì„¹ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ë¬¸ë‹¨(<p>)ë“¤ì„ ë°˜í™˜
    """
    url = f"https://{lang}.wikipedia.org/wiki/{title}"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"[âŒ] '{title}' í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (status {response.status_code})")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    # ğŸ¨ ì œëª© (ì–¸ë”ë°” â†’ ê³µë°±)
    clean_title = title.replace("_", " ")

    # ğŸ–¼ï¸ infobox ì´ë¯¸ì§€ URL ì¶”ì¶œ
    image_url = None
    infobox = soup.find("table", class_="infobox")
    if infobox:
        img = infobox.find("img")
        if img:
            image_url = "https:" + img["src"]


    # ğŸ“– ì„¹ì…˜ ë¬¸ë‹¨ ì¶”ì¶œ
    paragraphs = fetch_wikipedia_section_by_id(title, section_id, lang)

    return {
        "title": clean_title,
        "image_url": image_url,
        "paragraphs": paragraphs
    }

def fetch_wikipedia_section_by_id(title: str, section_id: str = "ì„¤ëª…", lang: str = "ko") -> list:
    """
    ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œì—ì„œ <h2 id=section_id>ê°€ ìˆëŠ” ì„¹ì…˜ ì•„ë˜ ë¬¸ë‹¨(<p>)ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    ë‹¤ìŒ ì„¹ì…˜(<div class="mw-heading2">)ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨.
    """
    url = f"https://{lang}.wikipedia.org/wiki/{title}"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"[âŒ] '{title}' í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (status {response.status_code})")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    # ëŒ€ìƒ h2 íƒœê·¸ (id="ì„¤ëª…") íƒìƒ‰
    target_h2 = soup.find("h2", id=section_id)
    if not target_h2:
        print(f"[âŒ] ì„¹ì…˜ ID '{section_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # í•´ë‹¹ <h2>ë¥¼ ê°ì‹¸ê³  ìˆëŠ” <div class="mw-heading2"> ìƒìœ„ ì„¹ì…˜ ë¸”ë¡
    section_div = target_h2.find_parent("div", class_="mw-heading2")
    if not section_div:
        print(f"[âŒ] ì„¹ì…˜ DIVë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # í˜•ì œ ë…¸ë“œ ì¤‘ ë‹¤ìŒ heading ì „ê¹Œì§€ì˜ <p> íƒœê·¸ ì¶”ì¶œ
    section_paragraphs = []
    for sibling in section_div.find_next_siblings():
        if sibling.name == "div" and "mw-heading2" in sibling.get("class", []):
            break
        if sibling.name == "p":
            section_paragraphs.append(sibling.get_text(strip=True))

    return section_paragraphs

# âœ… ì‚¬ìš© ì˜ˆì‹œ
title = "ì†Œí¬ë¼í…ŒìŠ¤ì˜_ì£½ìŒ"
result = fetch_wikipedia_info_with_section(title, section_id="ì„¤ëª…")

if result:
    print(f"ğŸ¨ ì œëª©: {result['title']}")
    print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ URL: {result['image_url'] or '[ì´ë¯¸ì§€ ì—†ìŒ]'}")
    print(f"ğŸ“– ì„¤ëª… ì„¹ì…˜ ë‚´ìš©:\n")

    if not result['paragraphs']:
        print("[ì„¤ëª… ì—†ìŒ]")
    else:
        for i, para in enumerate(result['paragraphs']):
            print(f"[ë¬¸ë‹¨ {i+1}]\n{para}\n")
