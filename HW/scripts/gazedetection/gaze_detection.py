# gazedetection.py
import os
import cv2
import dlib
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import joblib
from pathlib import Path
from typing import Optional, Union

# â”€â”€ ê¸°ë³¸ ì„¤ì •
SCREEN_ZONES = {1: "Top-Left", 2: "Top-Right", 3: "Bot-Left", 4: "Bot-Right"}
SAMPLES_PER_ZONE = 20

# â”€â”€ ë‚´ë¶€ ì „ì—­ (ì§€ì—° ì´ˆê¸°í™”ìš©)
_G = {
    "detector": None,
    "predictor": None,
    "model": None,
    "warned": False,
}

def _resolve_path(filename: str) -> Optional[Path]:
    """
    ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” í”í•œ ê²½ë¡œë¥¼ ìˆœíšŒ.
    """
    here = Path(__file__).resolve().parent
    cwd = Path.cwd()
    candidates = [
        cwd / filename,
        here / filename,
        cwd / "eyedia_model" / "data" / "gaze" / filename,
        here / "data" / "gaze" / filename,
        here.parent / "data" / "gaze" / filename,
        here.parent / "eyedia_model" / "data" / "gaze" / filename,
    ]
    for p in candidates:
        if p.is_file():
            return p
    return None

def _lazy_init_for_predict() -> bool:
    """
    predict_zone()ì—ì„œ ìµœì´ˆ 1íšŒë§Œ í˜¸ì¶œë˜ì–´ dlib predictorì™€ í•™ìŠµ ëª¨ë¸ ë¡œë“œ.
    """
    if _G["detector"] is not None and _G["predictor"] is not None and _G["model"] is not None:
        return True

    # dlib
    _G["detector"] = dlib.get_frontal_face_detector()

    sp_path = _resolve_path("shape_predictor_68_face_landmarks.dat")
    if sp_path is None:
        if not _G["warned"]:
            print("â„¹ï¸ Gaze predictor(.dat) íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ â†’ gaze ë¹„í™œì„±í™” (predict_zone -> None).")
            _G["warned"] = True
        return False
    _G["predictor"] = dlib.shape_predictor(str(sp_path))

    # í•™ìŠµ ëª¨ë¸
    gm_path = _resolve_path("gaze_model.pkl")
    if gm_path is None:
        if not _G["warned"]:
            print("â„¹ï¸ gaze_model.pklì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ â†’ gaze ë¹„í™œì„±í™” (predict_zone -> None).")
            _G["warned"] = True
        return False
    _G["model"] = joblib.load(str(gm_path))
    return True

def _get_eye_keypoints(shape, gray_frame, eye_points_indices):
    eye_points = np.array([(shape.part(i).x, shape.part(i).y) for i in eye_points_indices], dtype=np.int32)
    x, y, w, h = cv2.boundingRect(eye_points)
    if w == 0 or h == 0:
        return None, None, None, None
    eye_roi = gray_frame[y:y+h, x:x+w]

    inner_corner = (shape.part(eye_points_indices[3]).x, shape.part(eye_points_indices[3]).y)
    outer_corner = (shape.part(eye_points_indices[0]).x, shape.part(eye_points_indices[0]).y)

    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    eye_roi = clahe.apply(eye_roi)

    thr = cv2.adaptiveThreshold(eye_roi, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY_INV, 11, 2)
    contours, _ = cv2.findContours(thr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    pupil_contour = None
    max_circ = 0.0
    for c in contours:
        area = cv2.contourArea(c)
        if area == 0:
            continue
        peri = cv2.arcLength(c, True)
        if peri == 0:
            continue
        circ = 4 * np.pi * (area / (peri * peri))
        if 0.7 < circ < 1.2 and 15 < area < 400:
            if circ > max_circ:
                max_circ = circ
                pupil_contour = c

    pupil_center = None
    if pupil_contour is not None:
        M = cv2.moments(pupil_contour)
        if M["m00"] != 0:
            cx = int(M["m10"] / M["m00"]) + x
            cy = int(M["m01"] / M["m00"]) + y
            pupil_center = (cx, cy)

    glint_center = None
    if eye_roi.size > 0:
        _, max_val, _, max_loc = cv2.minMaxLoc(eye_roi)
        if max_val > 180:
            glint_center = (max_loc[0] + x, max_loc[1] + y)

    return inner_corner, outer_corner, pupil_center, glint_center

def _calc_features(left_eye, right_eye):
    if not all(p is not None for eye in [left_eye, right_eye] for p in eye):
        return None

    l_inner, _, l_pupil, l_glint = left_eye
    r_inner, _, r_pupil, r_glint = right_eye

    l_pupil, l_glint, l_inner = np.array(l_pupil), np.array(l_glint), np.array(l_inner)
    r_pupil, r_glint, r_inner = np.array(r_pupil), np.array(r_glint), np.array(r_inner)

    vl_pg = l_pupil - l_glint
    vr_pg = r_pupil - r_glint
    vl_pc = l_pupil - l_inner
    vr_pc = r_pupil - r_inner
    vl_gc = l_glint - l_inner
    vr_gc = r_glint - r_inner
    vcc = l_inner - r_inner

    def L(v): return np.linalg.norm(v)
    def ang(a, b): return np.arccos(
        np.clip(np.dot(a, b) / (L(a) * L(b) + 1e-6), -1.0, 1.0)
    )

    dist_cc = L(vcc)
    theta_l = ang(vl_pg, vl_gc)
    theta_r = ang(vr_pg, vr_gc)
    diff_cc = np.arctan2(vcc[1], vcc[0])

    feats = np.concatenate([
        vl_pg, [L(vl_pg)], vr_pg, [L(vr_pg)],
        vl_pc, [L(vl_pc)], vr_pc, [L(vr_pc)],
        vl_gc, [L(vl_gc)], vr_gc, [L(vr_gc)],
        [dist_cc, theta_l, theta_r, diff_cc]
    ])
    return feats

def predict_zone(frame_bgr: np.ndarray) -> Optional[int]:
    """
    [í•µì‹¬] í”„ë ˆì„ í•œ ì¥ì—ì„œ ì‹œì„  êµ¬ì—­ ì˜ˆì¸¡.
      - ì„±ê³µ: 1|2|3|4
      - ì‹¤íŒ¨: None (ëª¨ë¸/ì˜ˆì¸¡ ë¶ˆê°€/ì–¼êµ´ ì—†ìŒ ë“±)
    """
    if not _lazy_init_for_predict():
        return None

    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)
    faces = _G["detector"](gray)
    if len(faces) == 0:
        return None
    face = faces[0]
    landmarks = _G["predictor"](gray, face)

    left_idx = list(range(36, 42))
    right_idx = list(range(42, 48))
    left_eye = _get_eye_keypoints(landmarks, gray, left_idx)
    right_eye = _get_eye_keypoints(landmarks, gray, right_idx)

    feats = _calc_features(left_eye, right_eye)
    if feats is None:
        return None

    try:
        zone = int(_G["model"].predict([feats])[0])  # 1..4
        if zone in (1, 2, 3, 4):
            return zone
    except Exception:
        pass
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹´ë©”ë¼ ì—´ê¸° ë„ìš°ë¯¸ (index ë˜ëŠ” ê²½ë¡œ ëª¨ë‘ ì§€ì›, ê¸°ë³¸ /dev/video2)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _open_capture(src: Union[int, str, Path]) -> cv2.VideoCapture:
    """
    srcê°€ ì •ìˆ˜ë©´ ì¸ë±ìŠ¤ë¡œ, ë¬¸ìì—´/Pathë©´ ì¥ì¹˜ ê²½ë¡œë¡œ ì—´ê¸°.
    V4L2 ë°±ì—”ë“œ ì‚¬ìš©ì„ ê°•ì œ.
    """
    if isinstance(src, (str, Path)):
        src = str(src)
        cap = cv2.VideoCapture(src, cv2.CAP_V4L2)
    else:
        cap = cv2.VideoCapture(int(src), cv2.CAP_V4L2)
    return cap

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 'ìˆ˜ì§‘/í•™ìŠµ/ì‹œì—°'ì„ ìœ„í•œ ê°„ë‹¨í•œ CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run_collect_and_train(cam_src: Union[int, str, Path]):
    print("--- ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ (ìë™ ì§„í–‰) ---")
    print(f"ê° êµ¬ì—­ì„ ì‘ì‹œí•œ ìƒíƒœì—ì„œ 'ìŠ¤í˜ì´ìŠ¤ë°”'ë¥¼ ëˆŒëŸ¬ ë°ì´í„°ë¥¼ {SAMPLES_PER_ZONE}ê°œì”© ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print("ìˆ˜ì§‘ ì™„ë£Œ ì‹œ 's'ë¥¼ ëˆŒëŸ¬ í•™ìŠµ/ì €ì¥í•©ë‹ˆë‹¤.")
    print(f"ğŸ“· Using camera: {cam_src}")

    detector = dlib.get_frontal_face_detector()
    sp_path = _resolve_path("shape_predictor_68_face_landmarks.dat")
    if sp_path is None:
        print("âŒ predictor(.dat) íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    predictor = dlib.shape_predictor(str(sp_path))

    features_data, labels_data = [], []
    current_zone_to_collect = 1
    collected_counts = {i: 0 for i in range(1, 5)}

    cap = _open_capture(cam_src)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ/ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ í”„ë ˆì„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = detector(gray)

            features = None
            for face in faces:
                landmarks = predictor(gray, face)
                left_eye_keypoints = _get_eye_keypoints(landmarks, gray, list(range(36, 42)))
                right_eye_keypoints = _get_eye_keypoints(landmarks, gray, list(range(42, 48)))
                features = _calc_features(left_eye_keypoints, right_eye_keypoints)

            disp = np.zeros((720, 1280, 3), dtype=np.uint8)
            if current_zone_to_collect <= 4:
                cnt = collected_counts[current_zone_to_collect]
                text = f"Look at Zone [{current_zone_to_collect}] ({cnt}/{SAMPLES_PER_ZONE}). Press SPACE."
            else:
                text = "Collection Complete! Press 's' to train and save."
            cv2.putText(disp, text, (50, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)

            cv2.imshow("collect", disp)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            if key == ord(' ') and features is not None and current_zone_to_collect <= 4:
                if collected_counts[current_zone_to_collect] < SAMPLES_PER_ZONE:
                    features_data.append(features)
                    labels_data.append(current_zone_to_collect)
                    collected_counts[current_zone_to_collect] += 1
                    print(f"Zone {current_zone_to_collect} ({collected_counts[current_zone_to_collect]}/{SAMPLES_PER_ZONE})")
                if collected_counts[current_zone_to_collect] == SAMPLES_PER_ZONE:
                    current_zone_to_collect += 1

            if key == ord('s') and all(v == SAMPLES_PER_ZONE for v in collected_counts.values()):
                print("\n--- Training Model ---")
                X = np.array(features_data)
                y = np.array(labels_data)
                model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
                model.fit(X, y)
                joblib.dump(model, "gaze_model.pkl")
                print("âœ… Model saved: gaze_model.pkl")
                break
    finally:
        cap.release()
        cv2.destroyAllWindows()

def _run_predict_demo(cam_src: Union[int, str, Path]):
    print("--- ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë°ëª¨ (predict_zone ì‚¬ìš©) ---")
    print(f"ğŸ“· Using camera: {cam_src}")
    cap = _open_capture(cam_src)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ/ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ í”„ë ˆì„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                break
            zone = predict_zone(frame)
            text = f"Gaze Prediction: Zone {zone}" if zone else "Gaze Prediction: (None)"
            cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 2)
            cv2.imshow("gaze demo", frame)
            if (cv2.waitKey(1) & 0xFF) == ord('q'):
                break
    finally:
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    import argparse

    default_cam = (
    os.getenv("EYE_CAM")
    or os.getenv("CAM_DEV")
    or ("/dev/video-eye" if os.path.exists("/dev/video-eye") else None)
    or ("/dev/v4l/by-id/usb-Generic_USB2.0_PC_CAMERA-video-index0"
        if os.path.exists("/dev/v4l/by-id/usb-Generic_USB2.0_PC_CAMERA-video-index0") else None)
    or "/dev/video2"
    )
    p = argparse.ArgumentParser()
    p.add_argument("--mode", choices=["collect", "demo"], default="demo")
    p.add_argument(
        "--cam",
        default=default_cam,
        help="ì¹´ë©”ë¼ ì¸ë±ìŠ¤(ì˜ˆ: 0) ë˜ëŠ” ì¥ì¹˜ ê²½ë¡œ(ì˜ˆ: /dev/video2, /dev/v4l/by-id/...)"
    )
    args = p.parse_args()

    # ìˆ«ìë¡œ ë“¤ì–´ì˜¤ë©´ intë¡œ, ì•„ë‹ˆë©´ ë¬¸ìì—´ ê²½ë¡œë¡œ ì‚¬ìš©
    cam_src: Union[int, str]
    if isinstance(args.cam, str) and args.cam.isdigit():
        cam_src = int(args.cam)
    else:
        cam_src = args.cam  # path

    if args.mode == "collect":
        _run_collect_and_train(cam_src)
    else:
        _run_predict_demo(cam_src)
